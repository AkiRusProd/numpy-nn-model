import cupy as cp
import neunet

import os, sys

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from neunet.nn.experimental import CUDALinearSwish
from neunet.nn.layers import Linear
from neunet.nn.activations import Swish

def test_cutlass_linear_swish():
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    batch_size = 128
    in_features = 256
    out_features = 512
    device = "cuda"
    swish_beta = 1.0
    rtol = 1e-3
    atol = 1e-3
    # TF32 TensorOp tolerances for backward pass (TF32 has ~10-bit mantissa)
    bwd_rtol = 1e-3
    bwd_atol = 2e-3

    print(f"Testing Linear+Swish Layer: Batch={batch_size}, In={in_features}, Out={out_features}, Beta={swish_beta}")

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤
    # –°–æ–∑–¥–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Å–ª–æ–π (Linear + Swish)
    ref_linear = Linear(in_features, out_features, bias=True, device=device)
    ref_swish = Swish(beta=swish_beta)
    
    # –°–æ–∑–¥–∞–µ–º CUTLASS —Å–ª–æ–π (fused Linear+Swish)
    cutlass_layer = CUDALinearSwish(in_features, out_features, bias=True, swish_beta=swish_beta, device=device)

    # 2. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π (—á—Ç–æ–±—ã –æ–Ω–∏ –±—ã–ª–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã)
    cutlass_layer.weight.data = cp.copy(ref_linear.weight.data)
    cutlass_layer.bias.data = cp.copy(ref_linear.bias.data)

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    x_data = cp.random.uniform(-1, 1, (batch_size, in_features)).astype(cp.float32)
    X_ref = neunet.tensor(cp.copy(x_data), device=device, requires_grad=True)
    X_cutlass = neunet.tensor(cp.copy(x_data), device=device, requires_grad=True)

    # --- FORWARD PASS ---
    print("Running Forward pass...")
    # –≠—Ç–∞–ª–æ–Ω: Linear -> Swish
    out_ref = ref_swish(ref_linear(X_ref))
    # CUTLASS: fused Linear+Swish
    out_cutlass = cutlass_layer(X_cutlass)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Ö–æ–¥–∞
    forward_passed = cp.allclose(out_ref.data, out_cutlass.data, rtol=rtol, atol=atol)
    if forward_passed:
        print("‚úÖ Forward pass: SUCCESS")
    else:
        diff = cp.abs(out_ref.data - out_cutlass.data).max()
        print(f"‚ùå Forward pass: FAILED (Max diff: {diff})")
        print(f"   Ref range: [{out_ref.data.min():.6f}, {out_ref.data.max():.6f}]")
        print(f"   Cutlass range: [{out_cutlass.data.min():.6f}, {out_cutlass.data.max():.6f}]")

    # --- BACKWARD PASS ---
    print("Running Backward pass...")
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –≤—ã—Ö–æ–¥–∞
    grad_output = cp.random.uniform(-1, 1, out_ref.shape).astype(cp.float32)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º backprop –¥–ª—è —ç—Ç–∞–ª–æ–Ω–∞
    out_ref.backward(grad_output)
    # –ó–∞–ø—É—Å–∫–∞–µ–º backprop –¥–ª—è CUTLASS
    out_cutlass.backward(grad_output)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –≤—Ö–æ–¥—É (dX) ‚Äî uses TF32 TensorOp tolerances
    grad_X_passed = cp.allclose(X_ref.grad, X_cutlass.grad, rtol=bwd_rtol, atol=bwd_atol)
    if grad_X_passed:
        print("‚úÖ Backward X grad: SUCCESS")
    else:
        diff = cp.abs(X_ref.grad - X_cutlass.grad).max()
        print(f"‚ùå Backward X grad: FAILED (Max diff: {diff})")
        print(f"   Ref grad range: [{X_ref.grad.min():.6f}, {X_ref.grad.max():.6f}]")
        print(f"   Cutlass grad range: [{X_cutlass.grad.min():.6f}, {X_cutlass.grad.max():.6f}]")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –≤–µ—Å–∞–º (dW) ‚Äî uses TF32 TensorOp tolerances
    grad_W_passed = cp.allclose(ref_linear.weight.grad, cutlass_layer.weight.grad, rtol=bwd_rtol, atol=bwd_atol)
    if grad_W_passed:
        print("‚úÖ Backward Weight grad: SUCCESS")
    else:
        diff = cp.abs(ref_linear.weight.grad - cutlass_layer.weight.grad).max()
        print(f"‚ùå Backward Weight grad: FAILED (Max diff: {diff})")
        print(f"   Ref weight grad range: [{ref_linear.weight.grad.min():.6f}, {ref_linear.weight.grad.max():.6f}]")
        print(f"   Cutlass weight grad range: [{cutlass_layer.weight.grad.min():.6f}, {cutlass_layer.weight.grad.max():.6f}]")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–º–µ—â–µ–Ω–∏—é (db) ‚Äî uses TF32 TensorOp tolerances
    grad_b_passed = cp.allclose(ref_linear.bias.grad, cutlass_layer.bias.grad, rtol=bwd_rtol, atol=bwd_atol)
    if grad_b_passed:
        print("‚úÖ Backward Bias grad: SUCCESS")
    else:
        diff = cp.abs(ref_linear.bias.grad - cutlass_layer.bias.grad).max()
        print(f"‚ùå Backward Bias grad: FAILED (Max diff: {diff})")
        print(f"   Ref bias grad range: [{ref_linear.bias.grad.min():.6f}, {ref_linear.bias.grad.max():.6f}]")
        print(f"   Cutlass bias grad range: [{cutlass_layer.bias.grad.min():.6f}, {cutlass_layer.bias.grad.max():.6f}]")

    if all([forward_passed, grad_X_passed, grad_W_passed, grad_b_passed]):
        print("\n‚ú® ALL TESTS PASSED! CUTLASS Linear+Swish implementation is correct.")
        return True
    else:
        print("\n‚ö†Ô∏è SOME TESTS FAILED. Check alignment or layouts.")
        return False

def test_cutlass_linear_swish_different_beta():
    """–¢–µ—Å—Ç —Å –¥—Ä—É–≥–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º beta –¥–ª—è Swish"""
    batch_size = 64
    in_features = 128
    out_features = 256
    device = "cuda"
    swish_beta = 1.5  # –î—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ beta
    rtol = 1e-3
    atol = 1e-3
    # TF32 TensorOp tolerances for backward pass
    bwd_rtol = 1e-3
    bwd_atol = 2e-3

    print(f"\nTesting Linear+Swish with beta={swish_beta}: Batch={batch_size}, In={in_features}, Out={out_features}")

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤
    ref_linear = Linear(in_features, out_features, bias=True, device=device)
    ref_swish = Swish(beta=swish_beta)
    cutlass_layer = CUDALinearSwish(in_features, out_features, bias=True, swish_beta=swish_beta, device=device)

    # 2. –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    cutlass_layer.weight.data = cp.copy(ref_linear.weight.data)
    cutlass_layer.bias.data = cp.copy(ref_linear.bias.data)

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    x_data = cp.random.uniform(-1, 1, (batch_size, in_features)).astype(cp.float32)
    X_ref = neunet.tensor(cp.copy(x_data), device=device, requires_grad=True)
    X_cutlass = neunet.tensor(cp.copy(x_data), device=device, requires_grad=True)

    # Forward pass
    out_ref = ref_swish(ref_linear(X_ref))
    out_cutlass = cutlass_layer(X_cutlass)

    forward_passed = cp.allclose(out_ref.data, out_cutlass.data, rtol=rtol, atol=atol)
    if forward_passed:
        print(f"‚úÖ Forward pass with beta={swish_beta}: SUCCESS")
    else:
        diff = cp.abs(out_ref.data - out_cutlass.data).max()
        print(f"‚ùå Forward pass with beta={swish_beta}: FAILED (Max diff: {diff})")

    # Backward pass
    grad_output = cp.random.uniform(-1, 1, out_ref.shape).astype(cp.float32)
    out_ref.backward(grad_output)
    out_cutlass.backward(grad_output)

    all_passed = (
        forward_passed and
        cp.allclose(X_ref.grad, X_cutlass.grad, rtol=bwd_rtol, atol=bwd_atol) and
        cp.allclose(ref_linear.weight.grad, cutlass_layer.weight.grad, rtol=bwd_rtol, atol=bwd_atol) and
        cp.allclose(ref_linear.bias.grad, cutlass_layer.bias.grad, rtol=bwd_rtol, atol=bwd_atol)
    )

    if all_passed:
        print(f"‚úÖ All tests with beta={swish_beta}: SUCCESS")
    else:
        print(f"‚ùå Some tests with beta={swish_beta}: FAILED")

    return all_passed

if __name__ == "__main__":
    result1 = test_cutlass_linear_swish()
    result2 = test_cutlass_linear_swish_different_beta()
    
    if result1 and result2:
        print("\nüéâ ALL TESTS PASSED!")
    else:
        print("\n‚ùå SOME TESTS FAILED!")
        sys.exit(1)
