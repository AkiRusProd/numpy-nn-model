{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'has_changed_dir' not in globals():\n",
    "    repo_path = os.path.abspath(os.path.join('..'))\n",
    "    \n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    os.chdir(repo_path)\n",
    "    \n",
    "    globals()['has_changed_dir'] = True\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neunet\n",
    "import neunet.nn as nn\n",
    "from datasets import load_dataset  # type: ignore\n",
    "from neunet import Tensor\n",
    "from neunet.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Model implementation]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if d_model % n_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by n_heads\")\n",
    "\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor]=None):\n",
    "        batch_size = q.shape[0]\n",
    "        q = self.wq(q).contiguous().reshape(batch_size, -1, self.n_heads, self.depth).transpose(0, 2, 1, 3)\n",
    "        k = self.wk(k).contiguous().reshape(batch_size, -1, self.n_heads, self.depth).transpose(0, 2, 1, 3)\n",
    "        v = self.wv(v).contiguous().reshape(batch_size, -1, self.n_heads, self.depth).transpose(0, 2, 1, 3)\n",
    "\n",
    "        scores = neunet.matmul(q, k.transpose(0, 1, 3, 2)) / self.scale\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, ...]\n",
    "            scores = neunet.where(mask == 0, -1e9, scores)\n",
    "\n",
    "        attn = self.dropout(nn.Softmax(axis = -1)(scores))\n",
    "\n",
    "        x = neunet.matmul(attn, v)\n",
    "        x = x.contiguous().transpose(0, 2, 1, 3).reshape(batch_size, -1, self.n_heads * self.depth)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc_1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.Swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.RMSNorm(d_model)\n",
    "        self.norm2 = nn.RMSNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Masked self-attention (for the target sequence)\n",
    "        x = self.norm1(x)\n",
    "        _x, attn = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(_x)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        x = self.norm2(x)\n",
    "        _x = self.ffn(x)\n",
    "        x = x + self.dropout(_x)\n",
    "        \n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "            https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = neunet.zeros(max_len, d_model, requires_grad=False)\n",
    "        position = neunet.arange(0, max_len, dtype=neunet.float32)[:, None, ...]\n",
    "        div_term = neunet.exp(neunet.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = neunet.sin(position * div_term)\n",
    "        pe[:, 1::2] = neunet.cos(position * div_term)\n",
    "\n",
    "        self.pe = pe[None, ...]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]] # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, n_heads, d_ff, n_layers, dropout=0.1, max_len=5000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = math.sqrt(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.token_embedding(x) * self.scale\n",
    "        x = self.position_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, decoder: Decoder, pad_idx) -> None:\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def get_pad_mask(self, x):\n",
    "        #x: (batch_size, seq_len)\n",
    "        return (x != self.pad_idx).astype(int)[:, np.newaxis, :]\n",
    "\n",
    "    def get_sub_mask(self, x):\n",
    "        #x: (batch_size, seq_len)\n",
    "        seq_len = x.shape[1]\n",
    "        subsequent_mask = np.triu(np.ones((seq_len, seq_len)), k = 1).astype(int)\n",
    "        subsequent_mask = np.logical_not(subsequent_mask)\n",
    "        return subsequent_mask\n",
    "\n",
    "    def forward(self, x) -> tuple[Tensor, Tensor]:\n",
    "        # x: (batch_size, target_seq_len)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        mask = self.get_pad_mask(x) & self.get_sub_mask(x)\n",
    "\n",
    "        x, mask = neunet.tensor(x, dtype=neunet.int32, device=device), neunet.tensor(mask, dtype=neunet.int32, device=device)\n",
    "        \n",
    "\n",
    "        out, attention = self.decoder(x, mask)\n",
    "        # output: (batch_size, target_seq_len, vocab_size)\n",
    "        # attn: (batch_size, heads_num, target_seq_len, source_seq_len)\n",
    "        return out, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [Data preprocessing]\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "PAD_TOKEN = '<pad>' # noqa: S105\n",
    "SOS_TOKEN = '<sos>' # noqa: S105\n",
    "EOS_TOKEN = '<eos>' # noqa: S105\n",
    "\n",
    "\n",
    "DATASET_PATH = Path(\"./datasets/sd-prompts/\")\n",
    "SAVE_PATH = Path(\"./saved models/gpt/\")\n",
    "\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    data = load_dataset(\"Gustavosta/Stable-Diffusion-Prompts\", cache_dir=\"datasets/sd-prompts\")\n",
    "\n",
    "    for split, split_dataset in data.items():\n",
    "\n",
    "        with Path(f\"./datasets/sd-prompts/sd-prompts-{split}.txt\").open('w', encoding='utf-8') as f:\n",
    "            for item in split_dataset:\n",
    "                f.write(item['Prompt'] + '\\n')\n",
    "\n",
    "\n",
    "FILE_PATHS = [DATASET_PATH / \"sd-prompts-train.txt\", DATASET_PATH / \"sd-prompts-test.txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Train and load Tokenizer]\n",
    "\n",
    "if not (SAVE_PATH / \"vocab\").exists():\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    tokenizer.train(files=[str(path) for path in FILE_PATHS], vocab_size=15000, min_frequency=1, special_tokens=[\n",
    "        PAD_TOKEN,\n",
    "        SOS_TOKEN,\n",
    "        EOS_TOKEN,\n",
    "        # UNK_TOKEN\n",
    "    ])\n",
    "\n",
    "    (SAVE_PATH / \"vocab\").mkdir(parents=True)\n",
    "    tokenizer.save_model(str(SAVE_PATH / \"vocab\"), \"sd-prompts-tokenizer\")\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    str(SAVE_PATH / \"vocab/sd-prompts-tokenizer-vocab.json\"),\n",
    "    str(SAVE_PATH / \"vocab/sd-prompts-tokenizer-merges.txt\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "PAD_INDEX = tokenizer.token_to_id(PAD_TOKEN)\n",
    "SOS_INDEX = tokenizer.token_to_id(SOS_TOKEN)\n",
    "EOS_INDEX = tokenizer.token_to_id(EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataPreprocessor():\n",
    "    def __init__(self, tokenizer: ByteLevelBPETokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenizer._tokenizer.post_processor  = TemplateProcessing( # noqa SLF001\n",
    "            single=f\"{SOS_TOKEN} $A {EOS_TOKEN}\",\n",
    "            special_tokens=[\n",
    "                (f\"{SOS_TOKEN}\", tokenizer.token_to_id(f\"{SOS_TOKEN}\")),\n",
    "                (f\"{EOS_TOKEN}\", tokenizer.token_to_id(f\"{EOS_TOKEN}\")),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # self.tokenizer.enable_truncation(max_length=151)\n",
    "        self.tokenizer.enable_padding(pad_token = PAD_TOKEN)\n",
    "        \n",
    "    def tokenize(self, paths: list[str], batch_size: int, lines_limit: Optional[int] = None) -> list[np.ndarray]:\n",
    "        examples = []\n",
    "\n",
    "        for src_file in paths:\n",
    "            print(f\"Processing {src_file}\")\n",
    "            path_src_file = Path(src_file)\n",
    "            lines = path_src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "            if lines_limit:\n",
    "                lines = lines[:lines_limit]\n",
    "\n",
    "            for i in range(0, len(lines), batch_size):\n",
    "                examples.append(np.array([x.ids for x in self.tokenizer.encode_batch(lines[i:i+batch_size])]))\n",
    "\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def __call__(self, paths: list[str], batch_size: int, lines_limit: Optional[int] = None) -> list[np.ndarray]:\n",
    "        return self.tokenize(paths, batch_size, lines_limit)\n",
    "\n",
    "data_post_processor = DataPreprocessor(tokenizer)\n",
    "\n",
    "train_data = data_post_processor([str(FILE_PATHS[0])], batch_size = BATCH_SIZE, lines_limit=20000)\n",
    "val_data = data_post_processor([str(FILE_PATHS[1])], batch_size = BATCH_SIZE, lines_limit=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [Model intialization]\n",
    "\n",
    "decoder = Decoder(\n",
    "    tgt_vocab_size = tokenizer.get_vocab_size(),\n",
    "    d_model = 512,\n",
    "    n_heads = 8,\n",
    "    d_ff = 2048,\n",
    "    n_layers = 8,\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "\n",
    "model = GPT(\n",
    "    decoder = decoder,\n",
    "    pad_idx = tokenizer.token_to_id(PAD_TOKEN)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1.5e-4, betas=(0.9, 0.98), eps = 1e-9)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index = PAD_INDEX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [train, eval, predict methods definition]\n",
    "\n",
    "def train_step(dataset: list[np.ndarray], epoch: int, epochs: int) -> float:\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    tqdm_range = tqdm(enumerate(dataset), total = len(dataset))\n",
    "    for batch_num, batch in tqdm_range:\n",
    "\n",
    "        output, _ = model.forward(batch[:,:-1])\n",
    "        \n",
    "        output = output.reshape(output.shape[0] * output.shape[1], output.shape[2])\n",
    "\n",
    "        loss = loss_function(output, neunet.tensor(batch[:, 1:].flatten(), device=device, dtype=neunet.int32))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "\n",
    "        tqdm_range.set_description(\n",
    "                f\"training | loss: {loss.detach().item():.7f} | perplexity: {np.exp(loss.detach().item()):.7f} | epoch {epoch + 1}/{epochs}\" #loss: {loss:.4f}\n",
    "            )\n",
    "\n",
    "        if batch_num == (len(dataset) - 1):\n",
    "            epoch_loss = total_loss / len(dataset)\n",
    "\n",
    "            tqdm_range.set_description(\n",
    "                    f\"training | avg loss: {epoch_loss:.7f} | avg perplexity: {np.exp(epoch_loss):.7f} | epoch {epoch + 1}/{epochs}\"\n",
    "            )\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dataset: list[np.ndarray]) -> float:\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    tqdm_range = tqdm(enumerate(dataset), total = len(dataset))\n",
    "    for batch_num, batch in tqdm_range:\n",
    "        \n",
    "        output, _ = model.forward(batch[:,:-1])\n",
    "        \n",
    "        output = output.reshape(output.shape[0] * output.shape[1], output.shape[2])\n",
    "        \n",
    "        loss = loss_function(output, neunet.tensor(batch[:, 1:].flatten(), device=device, dtype=neunet.int32))\n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "        tqdm_range.set_description(\n",
    "                f\"testing  | loss: {loss.detach().item():.7f} | perplexity: {np.exp(loss.detach().item()):.7f}\"\n",
    "            )\n",
    "\n",
    "        if batch_num == (len(dataset) - 1):\n",
    "            epoch_loss = total_loss / len(dataset)\n",
    "\n",
    "            tqdm_range.set_description(\n",
    "                    f\"testing  | avg loss: {epoch_loss:.7f} | avg perplexity: {np.exp(epoch_loss):.7f}\"\n",
    "            )\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data: list[np.ndarray], val_data: list[np.ndarray], epochs: int, save_every_epochs: int, save_path: Optional[str] = None, validation_check: bool = False):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss_history.append(train_step(train_data, epoch, epochs))\n",
    "        val_loss_history.append(eval(val_data))\n",
    "\n",
    "\n",
    "        if (save_path is not None) and ((epoch + 1) % save_every_epochs == 0):\n",
    "            if not Path(save_path).exists():\n",
    "                Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "            if validation_check == False:\n",
    "\n",
    "                neunet.save(model.state_dict(), f\"{save_path}/gpt_{epoch + 1}.nt\")\n",
    "            else:\n",
    "                if val_loss_history[-1] < best_val_loss:\n",
    "                    best_val_loss = val_loss_history[-1]\n",
    "                    \n",
    "                    neunet.save(model.state_dict(), f\"{save_path}/gpt_{epoch + 1}.nt\")\n",
    "                else:\n",
    "                    print('Current validation loss is higher than previous. Not saved.')\n",
    "                    break\n",
    "            \n",
    "    return train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence: str = \"\", max_length: int = 50, temperature: float = 0.7) -> tuple[str, Tensor]:\n",
    "    model.eval()\n",
    "\n",
    "    tokens: list = [SOS_INDEX, *tokenizer.encode(sentence, add_special_tokens=False).ids]\n",
    "   \n",
    "    for _ in range(max_length):\n",
    "        inputs = np.asarray(tokens).reshape(1, -1)\n",
    "\n",
    "        outputs, attention = model.forward(inputs)\n",
    "\n",
    "        outputs = outputs[:, -1] / temperature\n",
    "        outputs = nn.Softmax(axis=-1)(outputs)\n",
    "\n",
    "        probs = outputs.reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "        next_token = np.random.choice(len(probs), p=probs)\n",
    "        \n",
    "        # next_token = outputs.detach().cpu().numpy().argmax(axis=-1)[:, -1].item() # eq. temperature = 0.0\n",
    "\n",
    "        tokens.append(next_token)\n",
    "\n",
    "        if next_token == EOS_INDEX or len(tokens) >= max_length:\n",
    "            break\n",
    "    \n",
    "    # Remove special tokens\n",
    "    if SOS_INDEX in tokens:\n",
    "        tokens.remove(SOS_INDEX)\n",
    "    if EOS_INDEX in tokens:\n",
    "        tokens.remove(EOS_INDEX)\n",
    "\n",
    "    decoded_sentence = tokenizer.decode(tokens)\n",
    "\n",
    "    return decoded_sentence, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Train the Model]\n",
    "\n",
    "# model.load_state_dict(neunet.load(\"./saved models/gpt/gpt_2.nt\"))\n",
    "\n",
    "train_loss_history, val_loss_history = None, None\n",
    "train_loss_history, val_loss_history = train(train_data, val_data, epochs=30, save_every_epochs=1, save_path = str(SAVE_PATH), validation_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Model inferecnce and Plot]\n",
    "\n",
    "def plot_loss_history(train_loss_history, val_loss_history):\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.plot(val_loss_history)\n",
    "    plt.title('Loss history')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "        \n",
    "if train_loss_history is not None and val_loss_history is not None:\n",
    "    plot_loss_history(train_loss_history, val_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_num = 10\n",
    "\n",
    "# [Generate sentences]\n",
    "for i in range(sentences_num):\n",
    "    print(f\"\\nExample â„–{i + 1}\")\n",
    "    print(f\"Generated sentence: {predict()[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
