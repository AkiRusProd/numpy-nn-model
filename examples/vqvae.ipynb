{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'has_changed_dir' not in globals():\n",
    "    repo_path = os.path.abspath(os.path.join('..'))\n",
    "    \n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    os.chdir(repo_path)\n",
    "    \n",
    "    globals()['has_changed_dir'] = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import neunet as nnet\n",
    "import neunet.nn as nn\n",
    "from data_loader import load_mnist\n",
    "from neunet.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_inputs = False\n",
    "\n",
    "image_size = (1, 28, 28)\n",
    "x_num, y_num = 5, 5\n",
    "samples_num = x_num * y_num\n",
    "margin = 15\n",
    "\n",
    "\n",
    "def add_noise(data):\n",
    "    noise_factor = 0.5\n",
    "\n",
    "    noisy_data = data + noise_factor * np.random.normal(0, 1, (data.shape))\n",
    "\n",
    "    return np.clip(noisy_data, 0, 1)\n",
    "\n",
    "\n",
    "training_data, test_data, training_labels, test_labels = load_mnist()\n",
    "training_data = training_data / 255  # normalization: / 255 => [0; 1]  #/ 127.5-1 => [-1; 1]\n",
    "test_data = test_data / 255  # normalization: / 255 => [0; 1]  #/ 127.5-1 => [-1; 1]\n",
    "\n",
    "num_embeddings = 100\n",
    "latent_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, num_embeddings):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(latent_size),\n",
    "        )\n",
    "\n",
    "        self.codebook = nn.Embedding(self.num_embeddings, self.latent_size)\n",
    "        self.codebook.weight = nnet.tensor(\n",
    "            np.random.uniform(\n",
    "                -1 / self.num_embeddings,\n",
    "                1 / self.num_embeddings,\n",
    "                (self.num_embeddings, self.latent_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, _ = self.quantize(z_e)\n",
    "        x_recon = self.decoder(z_q)\n",
    "        return x_recon, z_e, z_q\n",
    "\n",
    "    def quantize(self, z):\n",
    "        # distances = ((z[:, None, :] - self.codebook.weight) ** 2).sum(-1)\n",
    "        # z = z.reshape(-1, self.latent_size)\n",
    "        similarity = nnet.matmul(z, self.codebook.weight.T)\n",
    "        distances = (\n",
    "            nnet.sum(z**2, axis=1, keepdims=True)\n",
    "            + nnet.sum(self.codebook.weight**2, axis=1)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        min_indices = nnet.argmin(distances, axis=1)\n",
    "        z_q = self.codebook(min_indices)\n",
    "\n",
    "        return z_q, min_indices\n",
    "\n",
    "    def loss_function(self, x, x_recon, z_e, z_q, beta=0.25):\n",
    "        recon_loss = self.loss_fn(x_recon, x)\n",
    "        vq_loss = self.loss_fn(z_q, z_e.detach())\n",
    "        commit_loss = self.loss_fn(z_q.detach(), z_e)\n",
    "        return recon_loss + vq_loss + beta * commit_loss\n",
    "\n",
    "    def train_step(self, in_x, out_x, optimizer):\n",
    "        x_recon, z_e, z_q = self.forward(in_x)\n",
    "\n",
    "        loss = self.loss_function(out_x, x_recon, z_e, z_q)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def encode(self, x):\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, _ = self.quantize(z_e)\n",
    "        return z_q\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "\n",
    "vqvae = VQVAE(28 * 28, latent_size, num_embeddings)\n",
    "optimizer = Adam(vqvae.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_range = tqdm(range(0, len(training_data), batch_size), desc=\"epoch %d\" % epoch)\n",
    "    vqvae.train()\n",
    "    for i in tqdm_range:\n",
    "        batch = training_data[i : i + batch_size]\n",
    "\n",
    "        in_batch = nnet.tensor(batch).reshape(-1, 28 * 28)\n",
    "        if noisy_inputs:\n",
    "            in_batch = nnet.tensor(add_noise(in_batch.numpy()))\n",
    "\n",
    "        out_batch = nnet.tensor(batch).reshape(-1, 28 * 28)\n",
    "\n",
    "        loss = vqvae.train_step(in_batch, out_batch, optimizer)\n",
    "\n",
    "        tqdm_range.set_description(f\"epoch: {epoch + 1}/{epochs}, loss: {loss.data:.7f}\")\n",
    "\n",
    "    generated = (\n",
    "        vqvae.decode(nnet.tensor(np.random.normal(0, 1, size=(samples_num, latent_size))))\n",
    "        .detach()\n",
    "        .to(\"cpu\")\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    # samples = training_data[np.random.randint(0, len(training_data), samples_num)]\n",
    "    # if noisy_inputs:\n",
    "    #     samples = add_noise(samples)\n",
    "    # generated = vqvae.reconstruct(nnet.tensor(samples).reshape(-1, 28 * 28)).data\n",
    "    vqvae.eval()\n",
    "    for i in range(25):\n",
    "        image = generated[i] * 255\n",
    "        image = image.astype(np.uint8)\n",
    "        image = image.reshape(28, 28)\n",
    "        image = Image.fromarray(image)\n",
    "        image.save(f\"generated images/{i}.png\")\n",
    "\n",
    "\n",
    "vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_set(images):\n",
    "    images_array = np.full(\n",
    "        (y_num * (margin + image_size[1]), x_num * (margin + image_size[2])),\n",
    "        255,\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    num = 0\n",
    "    for i in range(y_num):\n",
    "        for j in range(x_num):\n",
    "            y = i * (margin + image_size[1])\n",
    "            x = j * (margin + image_size[2])\n",
    "\n",
    "            images_array[y : y + image_size[1], x : x + image_size[2]] = images[num]\n",
    "            num += 1\n",
    "\n",
    "    images_array = images_array[\n",
    "        : (y_num - 1) * (image_size[1] + margin) + image_size[1],\n",
    "        : (x_num - 1) * (image_size[2] + margin) + image_size[2],\n",
    "    ]\n",
    "\n",
    "    return Image.fromarray(images_array).convert(\"L\")\n",
    "\n",
    "\n",
    "samples = test_data[np.random.randint(0, len(test_data), samples_num)]\n",
    "if noisy_inputs:\n",
    "    samples = add_noise(samples)\n",
    "generated = vqvae.reconstruct(nnet.tensor(samples).reshape(-1, 28 * 28)).detach().to(\"cpu\").numpy()\n",
    "\n",
    "get_images_set(samples.reshape(-1, 28, 28) * 255).save(\"generated images/vqvae_in_samples.jpeg\")\n",
    "get_images_set(generated.reshape(-1, 28, 28) * 255).save(\"generated images/vqvae_out_samples.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize latent space only with latent_dim = 2\"\"\"\n",
    "\n",
    "\n",
    "def plot_latent_space_digits(n=30, figsize=15):\n",
    "    if latent_size != 2:\n",
    "        print(\"Can`t plot 2d latent space for non-2d latent space\")\n",
    "        return\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vqvae.decode(nnet.tensor(z_sample)).detach().to(\"cpu\").numpy()\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "\n",
    "    plt.savefig(\"generated images/vqvae 2d latent space.jpeg\")\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize latent space of labels only with latent_dim = 2\"\"\"\n",
    "\n",
    "\n",
    "def plot_label_clusters(data, labels):\n",
    "    if latent_size != 2:\n",
    "        print(\"Can`t plot 2d latent space for non-2d latent space\")\n",
    "        return\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean = vqvae.encode(nnet.tensor(data).reshape(-1, 28 * 28)).detach().to(\"cpu\").numpy()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "\n",
    "    plt.savefig(\"generated images/vqvae 2d latent space labels.jpeg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_clusters(add_noise(training_data) if noisy_inputs else training_data, training_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
